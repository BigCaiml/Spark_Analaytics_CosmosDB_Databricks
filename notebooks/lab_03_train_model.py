# Databricks notebook source
# MAGIC %md
# MAGIC ####Customer Profile Modeling
# MAGIC 
# MAGIC In this notebook, you will train a machine learning model, using customer demographic data to predict whether or not customer is likely to purchase a bicycle. The goal of this exercise is **not** to demonstrate a formal Data Science workflow (which typically involves extensive data exploration & visualization and feature selection & transformation) but instead to demonstrate the mechanics by which a model can be created and persisted in Databricks.
# MAGIC 
# MAGIC There are a number of ML frameworks available in Databricks. [Spark MLlib](https://spark.apache.org/mllib/), [Azure Machine Learning service](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-environment#azure-databricks), [MMLSpark](https://mmlspark.blob.core.windows.net/website/index.html), and others. In this lab, we will use MMLSpark, [Microsoft Machine Learning for Apache Spark](https://mmlspark.blob.core.windows.net/website/index.html). This library offers some useful extensions to PySpark to simplify both the modelling pipeline and a couple of ways to consider operationalization. To use this package, you will have to have installed the *Azure:mmlspark* library as directed in the environment setup.
# MAGIC 
# MAGIC Let's start first verifying the dataset from which we will train our model is accessible.  If the next cell returns an error, please run the *lab_01_setup_storage.py* notebook to land the file in the expected location.

# COMMAND ----------

# MAGIC %fs head /mnt/datasets/training/purchases.txt

# COMMAND ----------

# MAGIC %md Each line in this dataset provides demographic data long with a flag indicating whether (1) or not (0) the customer associated with this data purchased by bicycle within a given timeframe. You may recognize this dataset as one commonly used within the AdventureWorksDW sample database.
# MAGIC 
# MAGIC Let's now read this data into a dataframe. You may notice the data file contains a field named CustomerAlternateKey which uniquely identifies the customer represented by this row.  Having such data in the file can help us validate the dataset but is not useful for the purposes of building a predictive model.  We'll drop that field from our dataset:

# COMMAND ----------

df = spark.read.csv('dbfs:/mnt/datasets/training/purchases.txt', header=True, inferSchema=True, sep='\t').drop('CustomerAlternateKey')
display(df)

# COMMAND ----------

# MAGIC %md
# MAGIC As mentioned before, a proper Data Science workflow would involve careful feature selection & transformation.  As our goal is simply to produce a model - any model - we'll pretend the dataset is valid as is and begin with model training.
# MAGIC 
# MAGIC To do this, let's split the dataset into training and testing datasets:

# COMMAND ----------

train, test = df.randomSplit([0.75, 0.25], seed=123)

# COMMAND ----------

# MAGIC %md In the next cell, you will define and train a [Logistic Regression](https://spark.apache.org/docs/2.4.0/mllib-linear-methods.html#logistic-regression) model to generate predictions for the BikeBuyer column. The model definition will be saved to *dbfs:/mnt/datasets/model/bikeBuyer.mml* for use in batch scoring new customer profiles in later labs.

# COMMAND ----------

from mmlspark import TrainClassifier
from pyspark.ml.classification import LogisticRegression

# train a model using the training dataset  
lrmodel = TrainClassifier(
  model=LogisticRegression(), 
  labelCol='BikeBuyer').fit(train)

# save the trained model to storage
lrmodel.write().overwrite().save('dbfs:/mnt/datasets/model/bikeBuyer.mml')

# COMMAND ----------

# MAGIC %md In a normal Data Science workflow, we would typically validate the our model before deciding to use it.  As we are focused on the mechanics of model generation and persistence, this isn't necessary.  Still, we should take a quick look at some validation logic:

# COMMAND ----------

from mmlspark import ComputeModelStatistics

predictions = lrmodel.transform(test)
metrics = ComputeModelStatistics().transform(predictions)
display(metrics)

# COMMAND ----------

# MAGIC %md
# MAGIC The model statistics provide us some summary metrics on the accuracy and precision of our model.  We aren't going to scrutinize these too carefully.  Instead, let's look at the predictions made by it, captured in the predictions dataset:

# COMMAND ----------

display(predictions)

# COMMAND ----------

# MAGIC %md
# MAGIC The last three fields in the predictions dataset are the values predicted by our model:</p>
# MAGIC * **scores** - this value is the raw prediction score generated by the logistic regression model. These are used by the model to derive the score_probabilities. They are not used directly in the following labs. </p>
# MAGIC * **scored_probabilities** - this is 2-element vector of probabilies derived by evaluating the [logistic function](https://spark.apache.org/docs/2.4.0/mllib-linear-methods.html#logistic-regression) on the raw prediction column values. The first element is the probability the customer is a Non Bike Buyer. The second element is 1-the first, the probability the customer is a Bike Buyer.</p>
# MAGIC * **scored_labels** - this value is the Bike Buyer/Non Bike Buyer prediction based on a default probability threshold of 50%. Values over 50% are labeled as Bike Buyers (1) and values below 50% are labeled as Non Bike Buyers (0)</p>
# MAGIC 
# MAGIC Compare the scored_labels predictions to the actual BikeBuyer values.
# MAGIC 
# MAGIC The following labs will make use of the scored_probabilities values. The probability will be used to annotate each customer profile with their propensity to buy a bicycle. Using the probability directly enables you to set a higher threshold than 50% for determining that the customer should be shown bike-related products.

# COMMAND ----------

# MAGIC %md
# MAGIC You now have a saved model that can be used to generate a bike purchase propensity score for new customer profiles.

# COMMAND ----------

exit()

# COMMAND ----------

# MAGIC %md
# MAGIC ####SideBar - Persist your model as a real-time distributed web service
# MAGIC 
# MAGIC In our scenario, customer profile demographics are assumed not to change often, making batch scoring a better approach than real-time calls to the model. However, you may have a scenario where real-time scoring of new data is preferred, and this is something we can tackle using mmlspark's [Spark Serving](https://github.com/Azure/mmlspark/blob/master/docs/mmlspark-serving.md) functionality. Spark Serving enables model deployment as a web-service from within the Spark cluster.
# MAGIC 
# MAGIC To see this in action with the model we just trained, run the cell below:

# COMMAND ----------

from pyspark.sql.functions import col, from_json
from pyspark.sql.types import *
import uuid
from mmlspark import request_to_string, string_to_response

# define input data structure
serving_inputs = spark.readStream.server() \
  .address('localhost', 8898, 'my_api') \
  .load() \
  .parseRequest(test.schema)

# define output data structure
serving_outputs = lrmodel.transform(serving_inputs) \
  .makeReply('scored_probabilities')

# launch s
server = serving_outputs.writeStream \
  .server() \
  .replyTo('my_api') \
  .queryName('my_query') \
  .option('checkpointLocation', 'file:///tmp/checkpoints-{}'.format(uuid.uuid1())) \
  .start() 

# COMMAND ----------

# MAGIC %md
# MAGIC To test the service from python, we can send a single sample row of data in json format to the service:

# COMMAND ----------

# Test service with sample data
import requests, json

# input data
data = u'{"MaritalStatus":"M", "Gender":"F", "YearlyIncome":85000, "TotalChildren":2, "NumberChildrenAtHome":1,"Education":"Bachelors", "Occupation":"Professional", "HouseOwnerFlag":1, "NumberCarsOwned":1, "CommuteDistance":"5-10 Miles", "Region":"Pacific", "Age":28}'

# get score from web service
response = requests.post(data=data, url="http://localhost:8898/my_api")
score = json.loads(response.text)['scored_probabilities']['values']

# print score to screen
print( 'Customer propensity to purchase a bike: {0}'.format(score) )

# COMMAND ----------

# MAGIC %md ###IMPORTANT
# MAGIC 
# MAGIC Run the next cell to stop the web service. Otherwise, your cluster will not self-terminate.

# COMMAND ----------

# Stop the web service
server.stop()
